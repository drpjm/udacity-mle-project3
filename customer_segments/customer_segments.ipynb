{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Customer Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project you, will analyze a dataset containing annual spending amounts for internal structure, to understand the variation in the different types of customers that a wholesale distributor interacts with.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Run each code block below by pressing **Shift+Enter**, making sure to implement any steps marked with a TODO.\n",
    "- Answer each question in the space provided by editing the blocks labeled \"Answer:\".\n",
    "- When you are done, submit the completed notebook (.ipynb) with all code blocks executed, as well as a .pdf version (File > Download as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 440 rows, 6 columns\n",
      "   Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicatessen\n",
      "0  12669  9656     7561     214              2674          1338\n",
      "1   7057  9810     9568    1762              3293          1776\n",
      "2   6353  8808     7684    2405              3516          7844\n",
      "3  13265  1196     4221    6404               507          1788\n",
      "4  22615  5410     7198    3915              1777          5185\n",
      "(440, 6)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries: NumPy, pandas, matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Read dataset\n",
    "data = pd.read_csv(\"wholesale-customers.csv\")\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "print data.head()  # print the first 5 rows\n",
    "\n",
    "# Let's scale the data\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** In this section you will be using PCA and ICA to start to understand the structure of the data. Before doing any computations, what do you think will show up in your computations? List one or two ideas for what might show up as the first PCA dimensions, or what type of vectors will show up as ICA dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Since there are 6 features (the goods supplied to customers), running PCA on this data set should produce a set of 6 principal components. The component with the highest variance will be used by PCA to create a new \"x-axis\" for the data set coordinate system. Each of the remaining features will have an orthogonal axis associated with their variances.\n",
    "\n",
    "When applying ICA, we should expect the creation of new features that are mutually independent from each other. ICA computes a transformation matrix that combines the input features and generates new features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape = \n",
      "(440, 6)\n",
      "[[-0.018545    0.49047312  0.57644674 -0.01741587  0.647882    0.08213992]\n",
      " [-0.86387717 -0.18182434  0.00363933 -0.39690977  0.12652721 -0.21701704]\n",
      " [ 0.50178356 -0.33371506  0.08680857 -0.65833746  0.22070584 -0.38365676]\n",
      " [-0.02403365 -0.62796031  0.19740892  0.60521878  0.35050483 -0.27744285]\n",
      " [-0.00662714 -0.46794556  0.20894177 -0.20580297  0.05718506  0.83168587]\n",
      " [-0.03104894 -0.0412685   0.75995608 -0.01031851 -0.6240232  -0.17403508]]\n",
      "[ 0.51948323  0.26407007  0.106061    0.06258016  0.03497422  0.01283132]\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA with the same number of dimensions as variables in the dataset\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(data_scaled)\n",
    "print \"Original data shape = \"\n",
    "print data_scaled.shape\n",
    "\n",
    "# Print the components and the amount of variance in the data contained in each dimension\n",
    "print pca.components_\n",
    "print pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** How quickly does the variance drop off by dimension? If you were to use PCA on this dataset, how many dimensions would you choose for your analysis? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    "\n",
    "The first two components have the largest share of the variance ratio: ~51.9% and ~26.4%, respectively. The key hypothesis of PCA is that only a subset of the features in a data set really matter. I would use the first two dimensions, since their pricipal component eigenvalues are the largest (or, variance) and they can be use to compress the 6 dimensional feature space into just 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x113a553d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3uH0JScaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1138dcf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of data reduction: use the first two components...\n",
    "pca = PCA(n_components=2)\n",
    "Xt = pca.fit_transform(data_scaled)\n",
    "# Perform scaling...\n",
    "pc1 = Xt[:,0]\n",
    "pc2 = Xt[:,1]\n",
    "plt.scatter(pc1,pc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** What do the dimensions seem to represent? How can you use this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    "\n",
    "The principal components create a new coordinate system for the data set. The axes now represent *combinations* of the original features. This new set of features reduces the dimensionality of the problem, which will make a learning algorithm run more efficiently. It also shows how some of the dimensions may not really affect the overall data set. From the component ratios (pc1 = 0.45, pc2 = 0.40) and the plot above, it appears that the that a lot of the data is clustered near the origin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03383882 -0.1681465  -1.12313237  0.08883708  1.15184521  0.27495463]\n",
      " [-0.0171874  -0.72320928  0.53850906  0.02219931 -0.13467149  0.29052632]\n",
      " [ 0.04332757  0.01612148  0.05585404  0.03176465 -0.02087267 -0.86737852]\n",
      " [-0.02374098  0.13942493 -0.58863939 -0.02565546  0.02527595  0.06784086]\n",
      " [-0.44586744  0.06295564  0.05696113  0.04127184 -0.08323775  0.05028811]\n",
      " [ 0.09704244  0.0102899  -0.07185534 -0.67817012  0.02273911  0.28536851]]\n"
     ]
    }
   ],
   "source": [
    "# Fit an ICA model to the data\n",
    "# Data scaled from PCA...\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=6)\n",
    "ica.fit(data_scaled)\n",
    "\n",
    "# Print the independent components\n",
    "print ica.components_\n",
    "source_data = ica.fit_transform(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** For each vector in the ICA decomposition, write a sentence or two explaining what sort of object or property it corresponds to. What could these components be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    "\n",
    "ICA in general is attempting to reveal the *sources* of the data. It makes the assumption that the sources are statistically independent from each other and that they are the result of a linear combination of the sources. Applying the resulting ICA transformation would reveal the data signals.\n",
    "\n",
    "The transformation itself produces a new data set, where each element is the result of multiplying a row of the data (i.e. a customer) by a basis vector. For example: consider a row, r1 = [a b c d e f], and a basis vector in the ICA transformation t1 = [1 0 1 0 0 1]. Then r1 * t1' = [a+c+f].\n",
    "\n",
    "Looking at my specific basis transformation, I see that the first basis vector (column 1) has relatively small numbers for all elements except for element 2 (0.4458). In that case, Milk appears to be an important component of the source. Repeating for the remaining 5 columns we get:\n",
    "* A negative dependence on Detergents/Paper (-.723)\n",
    "* Negative dependence on Fresh (-1.12), then about an equal positive dependence on Grocery and Detergents/Paper\n",
    "* Negative dependence on Frozen (-.678)\n",
    "* Positive dependence on Fresh (1.15), and some negative dependence on Detergents/Paper\n",
    "* Negative dependence on Delicatessen (-.867), and some positive dependence on Frozen, Detergents/Paper, and Fresh.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "In this section you will choose either K Means clustering or Gaussian Mixed Models clustering, which implements expectation-maximization. Then you will sample elements from the clusters to understand their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Cluster Type\n",
    "\n",
    "**5)** What are the advantages of using K Means clustering or Gaussian Mixture Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** Below is some starter code to help you visualize some cluster data. The visualization is based on [this demo](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html) from the sklearn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import clustering modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: First we reduce the data to two dimensions using PCA to capture variation\n",
    "reduced_data = ?\n",
    "print reduced_data[:10]  # print upto 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement your clustering algorithm here, and fit it to the reduced data for visualization\n",
    "# The visualizer below assumes your clustering object is named 'clusters'\n",
    "\n",
    "clusters = ?\n",
    "print clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary by building a mesh grid to populate a graph.\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "hx = (x_max-x_min)/1000.\n",
    "hy = (y_max-y_min)/1000.\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Find the centroids for KMeans or the cluster means for GMM \n",
    "\n",
    "centroids = ?\n",
    "print centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)** What are the central objects in each cluster? Describe them as customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "** 8)** Which of these techniques did you feel gave you the most insight into the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**9)** How would you use that technique to help the company design new experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10)** How would you use that data to help you predict future customer needs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
